{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample dataset for testing naive Bayes\n",
    "\n",
    "    Returns:\n",
    "        pos_list: posts to be classified\n",
    "        labels: the label of each posting in pos_list\n",
    "    \"\"\"\n",
    "    pos_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "\n",
    "    labels = [0, 1, 0, 1, 0, 1]   # 1 is abusive, 0 is not\n",
    "    return pos_list, labels\n",
    "\n",
    "def create_vocab_list(dataset):\n",
    "    \"\"\"Create the vocabulary list for a given dataset\n",
    "\n",
    "    Args:\n",
    "        dataset: nested list of strings\n",
    "    Returns:\n",
    "        list of vocabulary of the dataset\n",
    "    \"\"\"\n",
    "    vocab_set = set()\n",
    "    for post in dataset:\n",
    "        vocab_set |= set(post)\n",
    "    return list(vocab_set)\n",
    "\n",
    "def word_2_vec_set(vocab_list, word_set):\n",
    "    \"\"\"Convert word set to one-hot vector. 1 stands for the word in vocab_list, otherwise 0.\n",
    "\n",
    "    Args:\n",
    "        vocab_list: the vocabulary list\n",
    "        word_set: set of words in a document or email\n",
    "    Retuns:\n",
    "        List of integers with the length of vocabulary list\n",
    "    \"\"\"\n",
    "    vec = [0] * len(vocab_list)\n",
    "    \n",
    "    for word in word_set:\n",
    "        if word in vocab_list:\n",
    "            vec[vocab_list.index(word)] = 1\n",
    "        else:\n",
    "            print(f\"The word: {word} is not in my Vocabulary!\")\n",
    "    \n",
    "    return vec\n",
    "\n",
    "def naive_bayes_fit(x_train, y_train):\n",
    "    \"\"\"Fit a naive Bayes model to the dataset.\n",
    "\n",
    "    Args:\n",
    "        x_train: Input one hot vectors. Ndarray of shape (m, n)\n",
    "        y_train: Labels of input. Array of shape (m,)\n",
    "    Returns:\n",
    "        p0_vec: p(x|y=0), a ndarray of hape (n,)\n",
    "        p1_vec: p(x|y=0) a ndarray of shape (n,)\n",
    "        p1: p(y=1). floating point\n",
    "    \"\"\"\n",
    "    m, n = x_train.shape   # to avoid product of probabilities = 0\n",
    "    p1 = sum(y_train) / m\n",
    "\n",
    "    p0_num = np.sum(x_train[y_train == 0], axis=0) + 1.0\n",
    "    p1_num = np.sum(x_train[y_train == 1], axis=0) + 1.0\n",
    "\n",
    "    p0_dem = x_train[y_train == 0].sum() + 2.0\n",
    "    p1_dem = x_train[y_train == 1].sum() + 2.0\n",
    "\n",
    "    p0_vec = np.log(p0_num / p0_dem)\n",
    "    p1_vec = np.log(p1_num / p1_dem)\n",
    "\n",
    "#     p0_num = np.sum(x_train[y_train == 0], axis=0)\n",
    "#     p1_num = np.sum(x_train[y_train == 1], axis=0)\n",
    "\n",
    "#     p0_dem = x_train[y_train == 0].sum()\n",
    "#     p1_dem = x_train[y_train == 1].sum()\n",
    "\n",
    "#     p0_vec = p0_num / p0_dem\n",
    "#     p1_vec = p1_num / p1_dem\n",
    "\n",
    "    return p0_vec, p1_vec, p1\n",
    "\n",
    "def classify_naive_bayes(x_vec, p0_vec, p1_vec, p1):\n",
    "    \"\"\"Using naive bayes to classify if a x vector is abusive or not.\n",
    "\n",
    "    Args: \n",
    "        x_vec: an one hot vector of a post. List of length n.\n",
    "        p0_vec: array of length n.\n",
    "        p1_vec: array of length n.\n",
    "        p1: p(y=1). floating point\n",
    "\n",
    "    Returns:\n",
    "        1: abusive post. 0: non-abusive post.\n",
    "    \"\"\"\n",
    "    x_arr = np.array(x_vec)\n",
    "    p0_y = x_arr.dot(p0_vec) + np.log(1 - p1)\n",
    "#     print(\"p0: \", p0_y)\n",
    "    p1_y = x_arr.dot(p1_vec) + np.log(p1)\n",
    "#     print(\"p1: \", p1_y)\n",
    "\n",
    "    if p1_y > p0_y: \n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def convert_dataset_to_array(dataset, labels, vocab_list):\n",
    "    m = len(dataset)\n",
    "    n = len(vocab_list)\n",
    "    x_list = []\n",
    "    \n",
    "    for i in range(m):\n",
    "        x_list.append(word_2_vec_bag(vocab_list, dataset[i]))\n",
    "    \n",
    "    x_train = np.array(x_list)\n",
    "    y_train = np.array(labels)\n",
    "    return x_train, y_train\n",
    "\n",
    "def word_2_vec_bag(vocab_list, word_list):\n",
    "    \"\"\"Convert word set to a bag of words vector.\n",
    "\n",
    "    Args:\n",
    "        vocab_list: the vocabulary list\n",
    "        word_list: list of words in a document or email\n",
    "    Retuns:\n",
    "        List of integers with the length of vocabulary list\n",
    "    \"\"\"\n",
    "    vec = [0] * len(vocab_list)\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word in vocab_list:\n",
    "            vec[vocab_list.index(word)] += 1    # increment 1 when word in vocabulary list\n",
    "        else:\n",
    "            print(f\"The word: {word} is not in my Vocabulary!\")\n",
    "    \n",
    "    return vec\n",
    "\n",
    "def text_to_token(text):\n",
    "    \"\"\"Convert a sentence into a list of words.\"\"\"\n",
    "    token_list = re.split(r'\\W+', text)\n",
    "    words = [token.lower() for token in token_list if len(token) > 2]\n",
    "    return words\n",
    "\n",
    "def spam_classifier():\n",
    "    \"\"\"Classify if an email is spam or not. 1: spam. 0: not spam.\"\"\"\n",
    "    # Create dataset & vocabulary list\n",
    "    dataset = []; labels = []; dataset = []\n",
    "\n",
    "    for i in range(25):\n",
    "        spam_text = open('email/spam/{}.txt'.format(i + 1), encoding='ISO-8859-1').read()\n",
    "        dataset.append(text_to_token(spam_text))\n",
    "        labels.append(1)\n",
    "\n",
    "        ham_text = open('email/ham/{}.txt'.format(i + 1), encoding='ISO-8859-1').read()\n",
    "        dataset.append(text_to_token(ham_text))\n",
    "        labels.append(0)\n",
    "    \n",
    "    vocab_list = create_vocab_list(dataset)\n",
    "    x_train, y_train = convert_dataset_to_array(dataset, labels, vocab_list)\n",
    "\n",
    "    # Fit model using training set\n",
    "    test = list(np.random.randint(0, 50, size = 10))\n",
    "    train = [x for x in range(50) if x not in test]\n",
    "\n",
    "    p0_vec, p1_vec, p_spam = naive_bayes_fit(x_train[train], y_train[train])            \n",
    "\n",
    "    # Validation using test set\n",
    "    err_cnt = 0\n",
    "    for i in range(10):\n",
    "        if classify_naive_bayes(x_train[test[i]], p0_vec, p1_vec, p_spam) \\\n",
    "            != y_train[test[i]]: \n",
    "            err_cnt += 1\n",
    "    print(\"The error rate is: \", float(err_cnt) / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydat, labels = create_sample_dataset()\n",
    "myvocab = create_vocab_list(mydat)\n",
    "x_train, y_train = create_data_ndarray(mydat, labels, myvocab)\n",
    "p0v, p1v, p1 = naive_bayes_fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04166667, 0.125     , 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "       0.        , 0.04166667, 0.        , 0.        , 0.        ,\n",
       "       0.04166667, 0.        , 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.        , 0.        , 0.        , 0.04166667, 0.08333333,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "       0.        , 0.04166667])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05263158, 0.        , 0.        , 0.05263158, 0.        ,\n",
       "       0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.05263158,\n",
       "       0.        , 0.15789474, 0.        , 0.        , 0.        ,\n",
       "       0.05263158, 0.05263158, 0.05263158, 0.        , 0.05263158,\n",
       "       0.        , 0.10526316, 0.        , 0.05263158, 0.        ,\n",
       "       0.10526316, 0.        ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.04761905,\n",
       "       0.0952381 , 0.04761905, 0.04761905, 0.0952381 , 0.04761905,\n",
       "       0.0952381 , 0.0952381 , 0.0952381 , 0.0952381 , 0.0952381 ,\n",
       "       0.04761905, 0.19047619, 0.04761905, 0.04761905, 0.04761905,\n",
       "       0.0952381 , 0.0952381 , 0.0952381 , 0.04761905, 0.0952381 ,\n",
       "       0.04761905, 0.14285714, 0.04761905, 0.0952381 , 0.04761905,\n",
       "       0.14285714, 0.04761905])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(p1v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error rate is:  0.1\n"
     ]
    }
   ],
   "source": [
    "spam_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny = feedparser.parse('http://feed.cnblogs.com/blog/sitehome/rss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
